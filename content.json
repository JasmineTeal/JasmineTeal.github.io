{"posts":[{"title":"ML2023Spring-hw3","text":"CNN TaskLink: ML2023Spring-hw3 | Kaggle 本次 homework 的任务是使用 CNN 进行图像分类，训练用的数据已经在文件名中标注好了对于的 label，代码中采用了 split 操作对文件路径进行分割并提取 label 为了达到更高的 baseline 在图像变换上使用了 TrivialAugmentWide，另外对损失函数 CrossEntropyLoss 添加了 label_smoothing 参数 Network StructureBrief Intro 1 input layer (implicit) 5 convolution layers (with ReLU, BatchNorm, MaxPool) 3 linear layers (with ReLU, Dropout) 1 output layer Core Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Classifier(nn.Module): def __init__(self): super(Classifier, self).__init__() # input size: [3, 128, 128] # 实际 size 是 [批量大小,通道数,高度,宽度] # padding 是在图像周围两边添加的像素数目 # padding_mode 补全的像素值 self.cnn = nn.Sequential( nn.Conv2d(3, 64, 3, 1, 1), # [64, 128, 128] nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [64, 64, 64] nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64] nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [128, 32, 32] nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32] nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [256, 16, 16] nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16] nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [512, 8, 8] nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8] nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [512, 4, 4] ) self.fc = nn.Sequential( nn.Dropout(0.5), nn.Linear(512 * 4 * 4, 1024), nn.ReLU(), nn.Dropout(0.5), nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, 11) ) def forward(self, x): out = self.cnn(x) out = out.view(out.size()[0], -1) # out.size()[0] is the batch size return self.fc(out) Learned Things nn.Conv2d 的参数中，padding 会在原图像周围一圈都进行填充；dilation 控制核元素之间的间距，可以在不增加卷积核大小的情况下扩展感受视野 Gradient Clipping 梯度裁剪是一种常用的技术，用于防止梯度爆炸问题；通过 nn.utils.clip_grad_norm_ 可以设置最大范数 max_norm，具体原理请 Google logits 通常指模型在经过最后一层线性变换后的输出值，它通常会经过激活函数（例如 Softmax）后再输出，但有时并不需要，会直接输出 DataLoader 中有一个 bool 参数 shuffle，通常在训练时设为 True，测试时设为 False，训练时可以使梯度更稳定、避免过拟合，减少数据顺序带来的影响 Data Augmentation 数据增强在图像分类任务中，本次代码中使用了 TrivialAugmentWide，详细可以参考这篇博客，对 torchvision.transforms 的用法进行了介绍 Test Time Augmentation 是指，在测试的时候也对测试数据进行增强，然后按一定方法与直接对测试数据进行预测的结果进行融合（例如加权平均） Full Code Private Score: 0.72600 Public score: 0.72800 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251import numpy as npimport pandas as pdimport torchimport osimport torch.nn as nnimport torch.utilsimport torch.utils.dataimport torchvision.transforms as transformsimport randomfrom PIL import Imagefrom torch.utils.data import Dataset, DataLoaderfrom torchvision.datasets import DatasetFolder, VisionDatasetfrom tqdm import tqdmdef set_seed(seed): torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False np.random.seed(seed) random.seed(seed)class FoodDataset(Dataset): def __init__(self, path, transform=None): super(FoodDataset).__init__() self.data = sorted([os.path.join(path, x) for x in os.listdir(path) if x.endswith(&quot;.jpg&quot;)]) self.transform = transform def __getitem__(self, idx): fname = self.data[idx] image = Image.open(fname) image = self.transform(image) try: label = int(fname.split(&quot;/&quot;)[-1].split(&quot;_&quot;)[0]) # string to int except: label = -1 # represent test has no label return image, label def __len__(self): return len(self.data) class Classifier(nn.Module): def __init__(self): super(Classifier, self).__init__() # input size: [3, 128, 128] # 实际 size 是 [批量大小,通道数,高度,宽度] # padding 是在图像周围两边添加的像素数目 # padding_mode 补全的像素值 self.cnn = nn.Sequential( nn.Conv2d(3, 64, 3, 1, 1), # [64, 128, 128] nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [64, 64, 64] nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64] nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [128, 32, 32] nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32] nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [256, 16, 16] nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16] nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [512, 8, 8] nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8] nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # [512, 4, 4] ) self.fc = nn.Sequential( nn.Dropout(0.5), nn.Linear(512 * 4 * 4, 1024), nn.ReLU(), nn.Dropout(0.5), nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, 11) ) def forward(self, x): out = self.cnn(x) out = out.view(out.size()[0], -1) # out.size()[0] is the batch size return self.fc(out)_exp_name = 'sample'seed = 42set_seed(seed)device = 'cuda' if torch.cuda.is_available() else 'cpu'print('Using {} device'.format(device))test_tfm = transforms.Compose([ transforms.Resize((128, 128)), transforms.ToTensor(),])train_tfm = transforms.Compose([ transforms.Resize((128, 128)), transforms.TrivialAugmentWide(), transforms.ToTensor(),])model = Classifier().to(device)batch_size = 64epochs = 50learning_rate = 0.0003patience = 10def train(): criterion = nn.CrossEntropyLoss(label_smoothing=0.1) optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5) train_set = FoodDataset(&quot;/kaggle/input/ml2023spring-hw3/train&quot;, transform=train_tfm) valid_set = FoodDataset(&quot;/kaggle/input/ml2023spring-hw3/valid&quot;, transform=test_tfm) train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True) valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True) update_step = 0 best_acc = 0 for epoch in range(epochs): model.train() train_losses = [] train_accs = [] for batch in tqdm(train_loader): imgs, labels = batch imgs = imgs.to(device) labels = labels.to(device) logits = model(imgs) # 在分类任务中，logits 是神经网络最后一层的线性输出 # 这些值随后会通过 softmax 函数转换为概率分布 loss = criterion(logits, labels) optimizer.zero_grad() loss.backward() # 梯度裁剪 grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10) optimizer.step() acc = (logits.argmax(dim=-1) == labels).float().mean() train_losses.append(loss.item()) train_accs.append(acc) train_loss = sum(train_losses) / len(train_losses) train_acc = sum(train_accs) / len(train_accs) print(f&quot;[ Train | {epoch + 1:03d}/{epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}&quot;) model.eval() valid_losses = [] valid_accs = [] for batch in tqdm(valid_loader): imgs, labels = batch imgs = imgs.to(device) labels = labels.to(device) with torch.no_grad(): logits = model(imgs) loss = criterion(logits, labels) acc = (logits.argmax(dim=-1) == labels).float().mean() valid_losses.append(loss.item()) valid_accs.append(acc) valid_loss = sum(valid_losses) / len(valid_losses) valid_acc = sum(valid_accs) / len(valid_accs) print(f&quot;[ Valid | {epoch + 1:03d}/{epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}&quot;) if valid_acc &gt; best_acc: with open(f&quot;./{_exp_name}_log.txt&quot;, &quot;a&quot;) as log_file: print(f&quot;[ Valid | {epoch + 1:03d}/{epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -&gt; best&quot;, file=log_file) else: with open(f&quot;./{_exp_name}_log.txt&quot;, &quot;a&quot;) as log_file: print(f&quot;[ Valid | {epoch + 1:03d}/{epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}&quot;, file=log_file) if valid_acc &gt; best_acc: print(f&quot;Best model found at epoch {epoch + 1}, saving model&quot;) torch.save(model.state_dict(), f&quot;{_exp_name}_best.ckpt&quot;) best_acc = valid_acc update_step = 0 else: update_step += 1 if update_step &gt; patience: print(f&quot;No improvment {patience} consecutive epochs, early stopping&quot;) break print(&quot;Finished Training&quot;) def test(): test_set = FoodDataset('/kaggle/input/ml2023spring-hw3/test', transform=test_tfm) test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True) # use Test Time Augmentation test_set_2 = FoodDataset('/kaggle/input/ml2023spring-hw3/test', transform=train_tfm) test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True) model = Classifier().to(device) model.load_state_dict(torch.load(f'{_exp_name}_best.ckpt')) model.eval() prediction = [] prediction_2 = [] with torch.no_grad(): for batch in tqdm(test_loader): imgs, _ = batch imgs = imgs.to(device) pred = model(imgs) prediction.append(pred) for batch in tqdm(test_loader_2): imgs, _ = batch imgs = imgs.to(device) pred = model(imgs) prediction_2.append(pred) predictions = 0.8 * torch.cat(prediction).cpu().numpy() + 0.2 * torch.cat(prediction_2).cpu().numpy() test_label = predictions.argmax(axis=-1) with open(f'./{_exp_name}_submission.csv', 'w') as f: f.write('Id,Category\\n') for i, pred in enumerate(test_label): f.write(f'{i},{pred}\\n') print('Finished Predicting')train()test()","link":"/2024/08/14/ML2023Spring-hw3/"},{"title":"Codeforces Round 961 (Div. 2)","text":"太久没写题了，脑力猛猛下降，后面题其实能做，但这段时间主力在学 Machine Learning， 有时间再补吧，去看李宏毅去了（逃 A. Diagonals题意$n \\times n$ 的棋盘上放 $k$ 个棋子，左下到右上共 $n$ 条对角线，问所有放置方案中最少有几条对角线被覆盖 题解简单模拟，写的时候脑子瘫痪了，第一反应没注意最后剩的不完整的一条对角线也算 代码1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;set&gt;#include &lt;queue&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;#define N (10010)#define inf (0x3f3f3f3f)#define INF (0x3f3f3f3f3f3f3f3fll)#define LL long longint T, n, k;inline int read() { int x = 0, f = 1; char ch = getchar(); while (ch &lt; '0' || ch &gt; '9') ch == '-' ? f = -1, ch = getchar() : ch = getchar(); while (ch &gt;= '0' &amp;&amp; ch &lt;= '9') x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (ch ^ 48), ch = getchar(); return x * f;}int main() { ios::sync_with_stdio(false); T = read(); while (T--) { n = read(), k = read(); int ans = 0; for (int i = n; i &gt;= 1 &amp;&amp; k; --i) { if (k &gt; i) k -= i, ++ans; else { ++ans; break; } if (i != n) { if (k &gt; i) k -= i, ++ans; else { ++ans; break; } } } printf(&quot;%d\\n&quot;, ans); } return 0;} B1. Bouquet (Easy Version)题意有 $n$ 朵花，每一朵花有 $a_i$ 个花瓣，购买它需要花费 $a_i$ 元，现在有 $m$ 元，要求购买的花里面，任意两朵之间的花瓣数差距不超过 $1$，问最多的花瓣数是多少 题解由于题意限制，可以认为只能购买 $x$ 和 $x+1$ 两种花瓣数的花朵，因此先对花瓣数相同的进行统计，假设购买 $k_1$ 朵 $x$ 的花，那么可以再购买 $\\left \\lfloor \\frac{m-k_1 \\times x}{k_1+1} \\right \\rfloor$ 朵 $x+1$ 的花，最后获得的总花瓣数可以计算得到 注意这道题在写的时候发现 unordered_map 的遍历始终有问题，本地 C++ 17 提交 C++ 14 运行后结果与本地不一致，后将 Codeforces 语言也设置为 C++ 17 后挂在了第 9 个点，随后改成 map 后再提交通过，狠狠 WA 了几发目前问题暂不明确，网上没有搜到相关信息 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;set&gt;#include &lt;queue&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;map&gt;using namespace std;#define N (200010)#define inf (0x3f3f3f3f)#define INF (0x3f3f3f3f3f3f3f3fll)#define int long longint T, n, m;map&lt;int, int&gt; a;inline int read() { int x = 0, f = 1; char ch = getchar(); while (ch &lt; '0' || ch &gt; '9') ch == '-' ? f = -1, ch = getchar() : ch = getchar(); while (ch &gt;= '0' &amp;&amp; ch &lt;= '9') x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (ch ^ 48), ch = getchar(); return x * f;}signed main() { ios::sync_with_stdio(false); T = read(); while (T--) { n = read(), m = read(); int tmp; for (int i = 1; i &lt;= n; ++i) tmp = read(), a[tmp]++; int Max = 0; for (auto x = a.begin(); x != a.end(); ++x) { int t = (*x).first; for (int i = 1; i &lt;= a[t]; ++i) { if (i * t &gt; m) continue; int res = m - i * t, j = 0; if (a[t + 1] != 0) j = min(a[t + 1], res / (t + 1)); Max = max(Max, i * t + j * (t + 1)); } } a.clear(); printf(&quot;%lld\\n&quot;, Max); } return 0;} B2. Bouquet (Hard Version)题意如上，但范围变大 题解不难发现，剩的钱越少，答案越大。如果先尽可能拿 $x$ 的花，然后再尽可能的拿 $x + 1$ 的花，如果还剩钱，就从剩的钱里面取 $1$ 元、少拿一个 $x$ 的花，这样就又有一个 $x + 1$ 的花了，这样的贪心策略显然可以使得剩的钱最少 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;set&gt;#include &lt;queue&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;#define N (200010)#define inf (0x3f3f3f3f)#define INF (0x3f3f3f3f3f3f3f3fll)#define int long longint T, n, m;struct Flower { int a, c; friend bool operator&lt; (const Flower &amp;a, const Flower &amp;b) { return a.a &lt; b.a; }} F[N];inline int read() { int x = 0, f = 1; char ch = getchar(); while (ch &lt; '0' || ch &gt; '9') ch == '-' ? f = -1, ch = getchar() : ch = getchar(); while (ch &gt;= '0' &amp;&amp; ch &lt;= '9') x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (ch ^ 48), ch = getchar(); return x * f;}signed main() { ios::sync_with_stdio(false); T = read(); while (T--) { n = read(), m = read(); for (int i = 1; i &lt;= n; ++i) F[i].a = read(); for (int i = 1; i &lt;= n; ++i) F[i].c = read(); sort(F + 1, F + n + 1); int ans = 0; for (int i = 1; i &lt;= n; ++i) { ans = max(ans, min(F[i].c, m / F[i].a) * F[i].a); int sum = 0, cnt1 = 0, cnt2 = 0, res; if (i != n &amp;&amp; F[i].a + 1 == F[i + 1].a) { // i != n is must! cnt1 = min(F[i].c, m / F[i].a); res = m - cnt1 * F[i].a; cnt2 = min(F[i + 1].c, res / F[i + 1].a); res = res - cnt2 * F[i + 1].a; sum = cnt1 * F[i].a + cnt2 * F[i + 1].a; sum += min(min(res, cnt1), F[i + 1].c - cnt2); ans = max(ans, sum); } } printf(&quot;%lld\\n&quot;, ans); } return 0;}","link":"/2024/07/25/Codeforces-Round-961-Div-2/"},{"title":"ML2023Spring-hw2","text":"Classification TaskLink: ML2023Spring-hw2 | Kaggle 本次 homework 的任务是根据音频文件，对每一个 phoneme 进行多分类任务，一共有 41 个类别，所有的数据预处理由 TA 给出，此处先不做任何学习、考虑。使用了 MLP 多层感知机作为网络主体 为了达到更高的 Baseline 采用了 BatchNorm、Dropout，并在 Sample 代码基础上增加了隐藏层维度和深度、提高了 concat_nframes 的数值 Network StructureBrief Intro 1 input layer (implicit) 6 linear layers (with ReLU, BatchNorm1d, Dropout) 1 output layer Core Code123456789101112131415161718192021222324252627282930class NerualBlock(nn.Module): def __init__(self, input_dim, output_dim, dropout_rate=0.5): super().__init__() self.linear = nn.Linear(input_dim, output_dim) self.bn = nn.BatchNorm1d(output_dim) self.act = nn.ReLU() self.dropout = nn.Dropout(dropout_rate) def forward(self, x): x = self.linear(x) x = self.bn(x) x = self.act(x) x = self.dropout(x) return xclass MLP(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim, hidden_layers=2, dropout_rate=0.15): super().__init__() self.fc = nn.Sequential( NerualBlock(input_dim, hidden_dim, dropout_rate), *[NerualBlock(hidden_dim, hidden_dim, dropout_rate) for _ in range(hidden_layers-1)], # 这里的 * 代表拆包，不加 * 意思就是一个列表，会报错 nn.Linear(hidden_dim, output_dim) ) def forward(self, x): x = self.fc(x) return x Learned Things 将隐藏层维度设置到大于原始数据维度，可能使结果更好 哪怕训练 epoch 比较小，修改网络深度也有可能使结果更好 一般情况下，Batchnorm 应该在激活函数之前。这种做法更能保持数据的稳定性，减少梯度消失或爆炸的风险，从而加速训练过程，提高模型的性能 最常见和推荐的做法是将 Dropout 放在激活函数之后（如 ReLU 之后）。这种位置可以更好地保持模型的非线性表达能力，同时通过随机丢弃部分神经元，防止模型过拟合。放在全连接层之后但激活函数之前的方式较少使用，而输入层之后的 Dropout 主要用于特定场景下的数据增强 Full Code Private Score: 0.73510 Public score: 0.73447 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308import osimport numpy as npimport torchimport randomfrom torch.utils.data import Dataset, DataLoaderfrom tqdm import tqdmimport torch.nn as nndef set_seed(seed): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) torch.backends.cudnn.benchmark = True torch.backends.cudnn.deterministic = True&quot;&quot;&quot; Fixes random number generator seeds for reproducibility. &quot;&quot;&quot;def load_feature(path): feature = torch.load(path) return feature&quot;&quot;&quot; Loads features. &quot;&quot;&quot;def shift(x, n): if n &lt; 0: left = x[0].repeat(-n, 1) right = x[:n] elif n &gt; 0: right = x[-1].repeat(n, 1) left = x[n:] else: return x return torch.cat((left, right), dim=0)&quot;&quot;&quot; Shifts features. &quot;&quot;&quot;def concat_feat(x, concat_n): assert concat_n % 2 == 1 # n must be odd if concat_n &lt; 2: return x seq_len, feature_dim = x.size(0), x.size(1) x = x.repeat(1, concat_n) x = x.view(seq_len, concat_n, feature_dim).permute(1, 0, 2) &quot;&quot;&quot; permute(1, 0, 2)表示将原始张量的第一个维度移动到第二个位置， 第二个维度移动到第一个位置，而第三个维度保持不变 &quot;&quot;&quot; mid = concat_n // 2 # 整除 for r_idx in range(1, mid+1): x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx) x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx) return x.permute(1, 0, 2).view(seq_len, concat_n * feature_dim)&quot;&quot;&quot; Concatenates features. &quot;&quot;&quot;def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8): class_num = 41 if split == 'train' or split == 'val': mode = 'train' elif split == 'test': mode = 'test' else: raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!') label_dict = {} if mode == 'train': for line in open (os.path.join(phone_path, 'train_labels.txt')).readlines(): # readlines() 方法将文件的每一行读入一个列表中，每个元素是文件中的一行。 line = line.strip('\\n').split(' ') label_dict[line[0]] = [int (p) for p in line[1:]] # split training and validation data usage_list = open(os.path.join(phone_path, 'train_split.txt')).readlines() random.shuffle(usage_list) train_len = int(len(usage_list) * train_ratio) if split == 'train': usage_list = usage_list[:train_len] else: usage_list = usage_list[train_len:] elif mode == 'test': usage_list = open(os.path.join(phone_path, 'test_split.txt')).readlines() usage_list = [line.strip('\\n') for line in usage_list] print('[Dataset] - # phone classes: ' + str(class_num) + ', number of utterances for ' + split + ': ' + str(len(usage_list))) max_len = 3000000 X = torch.empty(max_len, 39 * concat_nframes) if mode == 'train': y = torch.empty(max_len, dtype=torch.long) idx = 0 for _, fname in tqdm(enumerate(usage_list)): feat = load_feature(os.path.join(feat_dir, mode, fname + '.pt')) cur_len = len(feat) feat = concat_feat(feat, concat_nframes) if mode == 'train': label = torch.LongTensor(label_dict[fname]) X[idx: idx + cur_len, :] = feat if mode == 'train': y[idx: idx + cur_len] = label idx = idx + cur_len X = X[:idx, :] if mode == 'train': y = y[:idx] print(f'[INFO] {split} set') print(X.shape) if mode == 'train': print(y.shape) return X, y else: return Xclass LibriDataset(Dataset): def __init__(self, X, y=None): self.data = X self.label = torch.LongTensor(y) if y is not None else None # LongTensor 不接受 None def __getitem__(self, idx): if self.label is not None: return self.data[idx], self.label[idx] else: return self.data[idx] def __len__(self): return len(self.data) class NerualBlock(nn.Module): def __init__(self, input_dim, output_dim, dropout_rate=0.5): super().__init__() self.linear = nn.Linear(input_dim, output_dim) self.bn = nn.BatchNorm1d(output_dim) self.act = nn.ReLU() self.dropout = nn.Dropout(dropout_rate) def forward(self, x): x = self.linear(x) x = self.bn(x) x = self.act(x) x = self.dropout(x) return xclass MLP(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim, hidden_layers=2, dropout_rate=0.15): super().__init__() self.fc = nn.Sequential( NerualBlock(input_dim, hidden_dim, dropout_rate), *[NerualBlock(hidden_dim, hidden_dim, dropout_rate) for _ in range(hidden_layers-1)], # 这里的 * 代表拆包，不加 * 意思就是一个列表，会报错 nn.Linear(hidden_dim, output_dim) ) def forward(self, x): x = self.fc(x) return x# data parametersconcat_nframes = 19train_ration = 0.9# training parametersseed = 42batch_size = 512num_epoch = 20lr = 1e-3model_path = 'model.ckpt'# model parametersinput_dim = 39 * concat_nframeshidden_dim = 512output_dim = 41hidden_layers = 6set_seed(seed) def train(): device = 'cuda' if torch.cuda.is_available() else 'cpu' print(f'device = {device}') train_X, train_y = preprocess_data(split='train', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ration) val_X, val_y = preprocess_data(split='val', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ration) train_set = LibriDataset(train_X, train_y) val_set = LibriDataset(val_X, val_y) # remove raw feature to save memory import gc del train_X, train_y, val_X, val_y gc.collect() train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True) val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False) model = MLP(input_dim, hidden_dim, output_dim, hidden_layers).to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=lr) best_acc = 0.0 early_stopping = 0 for epoch in range(num_epoch): train_acc = 0.0 train_loss = 0.0 val_acc = 0.0 val_loss = 0.0 model.train() for batch in tqdm(train_loader): inputs, labels = batch # 每个 batch 会是一个元组 (inputs, labels) inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() _, train_pred = torch.max(outputs.detach(), 1) # get the index of the class with the highest probability # torch.max 函数有两个输出值：最大值和最大值的索引 train_acc += (train_pred == labels).sum().item() train_loss += loss.item() train_acc = train_acc / len(train_set) train_loss = train_loss / len(train_loader) # 计算的 loss 是每个 batch 的平均值 # 在每个 epoch 末尾，进行一次验证 model.eval() with torch.no_grad(): for batch in tqdm(val_loader): inputs, labels = batch inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) loss = criterion(outputs, labels) _, val_pred = torch.max(outputs, 1) # 可以将数值移到 cpu 中，然后再进行比较 # # 将 val_pred 和 labels 移动到 CPU 上进行比较 val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability val_loss += loss.item() val_acc = val_acc / len(val_set) val_loss = val_loss / len(val_loader) print(f'[{epoch+1:03d}/{num_epoch:03d}] Train Acc: {train_acc:.5f} | Train Loss: {train_loss:.5f} | Val Acc: {val_acc:.5f} | Val Loss: {val_loss:.5f}') if val_acc &gt; best_acc: best_acc = val_acc torch.save(model.state_dict(), model_path) print(f'saving model with acc {best_acc:.5f}') early_stopping = 0 else: early_stopping += 1 if early_stopping &gt; 10: print(f'early stopping with best acc {best_acc:.5f}') break del train_set, val_set del train_loader, val_loader gc.collect() def predict(): test_X = preprocess_data(split='test', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes) test_set = LibriDataset(test_X, None) test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False) device = 'cuda' if torch.cuda.is_available() else 'cpu' model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, hidden_layers=hidden_layers).to(device) model.load_state_dict(torch.load(model_path)) model.eval() pred_list = [] with torch.no_grad(): for batch in tqdm(test_loader): inputs = batch inputs = inputs.to(device) outputs = model(inputs) _, test_pred = torch.max(outputs, 1) pred_list.append(test_pred.cpu().numpy()) pred_list = np.concatenate(pred_list) with open('prediction.csv', 'w') as f: f.write('Id,Class\\n') for i, p in enumerate(pred_list): f.write('{},{}\\n'.format(i, p)) if __name__ == '__main__': train() predict()","link":"/2024/08/09/ML2023Spring-hw2/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/04/30/hello-world/"},{"title":"ML2023Spring-hw1","text":"Regression TaskLink: ML2023Spring-hw1 | Kaggle 本次 homework 的任务是根据 COVID-19 Cases 进行 Prediction，主要使用了线性全连接层和 ReLU 激活函数构建网络 为了达到更高的 Baseline 采用了特征截取、Adam 学习器、L2 正则化（weight decay in optimizer）并在 Sample 代码基础上增加了网络深度 Network StructureBrief Intro 1 input layer (implicit) 3 linear layers (with ReLU activation function) 1 output layer Core Code1234567891011121314151617class LinearModel(nn.Module): def __init__(self, inputs): super(LinearModel, self).__init__() self.layer1 = nn.Sequential( nn.Linear(inputs, 16), nn.ReLU(), nn.Linear(16, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 1) ) def forward(self, x): x = self.layer1(x) x = x.squeeze(1) # (X, 1) -&gt; (X) return x Learned Things这篇笔记是事后补的，记忆有些凌乱，以下记录不具有任何逻辑顺序 事先定义一个设置随机数种子的函数以确保每次运行结果一致 有些用于评估的变量记得添加 detach 使其从计算图中分离 数据读入三步走：划分训练集和验证集、将 data 和 label 通过 Dataset 打包、通过 DataLoader 将 dataset 创建维 loader 供训练使用 epochs 较多时可以采用早停法提高训练效率 数据提前进行特征数据筛选/截取，将明显无关的数据扔掉 Full Code Private Score: 0.84523 Public score: 0.82729 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210# Numerical Operationsimport mathimport numpy as np# Reading/Writing Dataimport pandas as pdimport osimport csv# For Progress Barfrom tqdm import tqdm# Pytorchimport torch import torch.nn as nnfrom torch.utils.data import Dataset, DataLoader, random_split# For plotting learning curvefrom torch.utils.tensorboard import SummaryWriterclass MyDataset(Dataset): def __init__(self, x, y=None): if y is not None: self.y = torch.tensor(y, dtype=torch.float32) self.x = torch.tensor(x, dtype=torch.float32) def __getitem__(self, index): if hasattr(self, 'y'): return self.x[index], self.y[index] else: return self.x[index] def __len__(self): return len(self.x) class LinearModel(nn.Module): def __init__(self, inputs): super(LinearModel, self).__init__() self.layer1 = nn.Sequential( nn.Linear(inputs, 16), nn.ReLU(), nn.Linear(16, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 1) ) def forward(self, x): x = self.layer1(x) x = x.squeeze(1) # (X, 1) -&gt; (X) return xdef set_seed(seed): np.random.seed(seed) torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)def split_data(dataset, valid_rate, seed): valid_size = int(valid_rate * len(dataset)) train_size = len(dataset) - valid_size train_set, valid_set = random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(seed)) return np.array(train_set), np.array(valid_set)def predict(test_loader, model, device): model.eval() preds = [] for x in tqdm(test_loader): x = x.to(device) with torch.no_grad(): pred = model(x) preds.append(pred.detach().cpu()) # detach 的作用是将张量从这个计算图中分离出来， # 使其不再跟踪计算图中的操作 preds = torch.cat(preds, dim=0).numpy() return predsdef select_feat(train_data, valid_data, test_data, select_all=True): y_train, y_valid = train_data[:,-1], valid_data[:,-1] raw_x_train, raw_x_valid, raw_x_test = train_data[:, :-1], valid_data[:, :-1], test_data if select_all: feat_idx = list(range(raw_x_train.shape[1])) else: feat_idx = list(range(35, raw_x_train.shape[1])) return raw_x_train[:, feat_idx], raw_x_valid[:, feat_idx], raw_x_test[:, feat_idx], y_train, y_validdef train_model(train_loader, valid_loader, model, args, device): critertion = nn.MSELoss(reduction='mean') optimizer = torch.optim.Adam(model.parameters(), lr=args['learning_rate'], weight_decay=1e-4) writer = SummaryWriter() if not os.path.exists('./models'): os.mkdir('./models') epochs, best_loss, step, early_stop = args['epochs'], math.inf, 0, 0 for i in range(epochs): model.train() loss_sum = [] tarin_process_bar = tqdm(train_loader, position=0, leave=True) for x, y in tarin_process_bar: optimizer.zero_grad() x, y = x.to(device), y.to(device) pred = model(x) loss = critertion(pred, y) loss.backward() optimizer.step() step += 1 loss_sum.append(loss.detach().item()) tarin_process_bar.set_description(f'Epoch [{i+1}/{epochs}]') tarin_process_bar.set_postfix({'loss': loss.detach().item()}) mean_train_loss = sum(loss_sum) / len(loss_sum) writer.add_scalar('loss/train', mean_train_loss, step) model.eval() loss_sum = [] for x, y in valid_loader: x, y = x.to(device), y.to(device) with torch.no_grad(): pred = model(x) loss = critertion(pred, y) loss_sum.append(loss.item()) mean_valid_loss = sum(loss_sum) / len(loss_sum) print(f'Epoch [{i+1}/{epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}') writer.add_scalar('loss/valid', mean_valid_loss, step) if mean_valid_loss &lt; best_loss: best_loss = mean_valid_loss torch.save(model.state_dict(), args['save_path']) print(f'Saving model with loss {best_loss:.3f}') early_stop = 0 else: early_stop += 1 if (early_stop == args['early_stop']): print('\\nModel is not improving, so we stop training') return def save_pred(preds, file): ''' Save predictions to specified file ''' with open(file, 'w', newline='') as fp: writer = csv.writer(fp) writer.writerow(['id', 'tested_positive']) for i, p in enumerate(preds): writer.writerow([i, p])def main(args): device = 'cuda' if torch.cuda.is_available() else 'cpu' set_seed(args['seed']) # train_data size: 3009 x 89 # test_data size: 997 x 88 train_data, test_data = pd.read_csv('./covid_train.csv').values, pd.read_csv('./covid_test.csv').values train_data, valid_data = split_data(train_data, args['valid_ratio'], args['seed']) print(f'train_data size: {train_data.shape}\\nvalid_data size: {valid_data.shape}\\ntest_data size: {test_data.shape}') x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, args['select_all']) train_dataset, valid_dataset, test_dataset = MyDataset(x_train, y_train), \\ MyDataset(x_valid, y_valid), \\ MyDataset(x_test) # Pytorch data loader loads pytorch dataset into batches. train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, pin_memory=True) valid_loader = DataLoader(valid_dataset, batch_size=args['batch_size'], shuffle=True, pin_memory=True) test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False, pin_memory=True) # print(train_loader) model = LinearModel(inputs=x_train.shape[1]).to(device) train_model(train_loader, valid_loader, model, args, device) model.load_state_dict(torch.load(args['save_path'])) output = predict(test_loader, model, device) print(output) save_pred(output, 'pred.csv') if __name__ == '__main__': args = { 'seed': 42, 'select_all': False, 'valid_ratio': 0.2, 'epochs': 3000, 'batch_size': 128, 'learning_rate': 1e-3, 'early_stop': 600, 'save_path': './models/model.ckpt' } main(args)","link":"/2024/07/24/ML2023Spring-hw1/"},{"title":"Machine Learning Notes","text":"主要记录个人学习李宏毅的 Machine Learning 课程的笔记，其实更像是随笔，写的比较随意，会写一些概念理论，但更多会写令自己豁然开朗的部分。另外本篇文章不时更新，如果本篇内容有帮助到你，我会感到非常荣幸！ Deep LearningCourses 【生成式AI】快速了解機器學習基本原理 (1/2) (已經略懂機器學習的同學可以跳過這段) - YouTube 【生成式AI】快速了解機器學習基本原理 (2/2) (已經略懂機器學習的同學可以跳過這段) (youtube.com) ML Lecture 6: Brief Introduction of Deep Learning - YouTube ML Lecture 3-1: Gradient Descent - YouTube ML Lecture 7: Backpropagation (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point) (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum) (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate) (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響 (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介 (youtube.com) Notes ML or DL 都可以理解为三个步骤： Define a function set: Deep Learning(CNN, Transformer), Decision Tree, etc. Goodness of function: Supervised Learning, Semi-supervised Learning, Reinforced Learning, etc. Pick the best function: Gradient Descent(Adam, AdamW), Genetic Algorithm, etc. Gradient Descent 就是用总的 Loss 对网络中每个权重和偏置求偏导，然后用这个参数减去即可，即 $w_{1}-\\mu \\frac{\\partial L}{\\partial w_{1}}$ ，其中 $\\mu$ 表示 Learning Rate，通常来说，$\\eta$ 要么为一开始的固定值，要么为一个关于 $t$ 的函数，并且通常随 $t$ 增大而变小 训练模型时，最后把 epochs / loss 的折线图输出，方便观察当前 learning rate 下参数变化导致的 loss 变化趋势，以便调整 learning rate Learning Rate 不仅跟一阶微分成正比，还和二阶微分成反比，即 $\\frac{ \\left| First \\space Derivative \\right|}{Second \\space Derivative}$ Adaptive Learning Rates 就是说针对每个参数都给一个独立的 Learning Rate，比如 Adagrad，将原始的参数更新式修改为 $w_{t}-\\frac{\\eta^{t}}{\\sigma^{t}}g^{t}$ ，其中 $\\sigma^t$ : root mean square of the previous derivatives of parameter $w$ ，$\\eta^{t}$ : time $t$’s learning rate，$\\sigma^t=\\sqrt{\\frac{1}{t+1}\\sum_{i=0}^{t}(g^{i})^{2}}$ ，Adagrad 反映了每次的梯度的反差大小 Stochastic Gradient Descent 简单来说就是不求所有 sample 的 loss 和，随机选择一个 sample 计算它的 loss 和 gradient 作为梯度下降的依据，更新所有参数；传统梯度下降会把 $n$ samples 看一遍然后更新一次，这种做法看完 $n$ samples 后总共会更新 $n$ 次 对于数据做 Feature Scaling(Normalization) 是有必要的，这可以使训练时的 learning rate 相对统一（每一种数据对参数的影响相当），通常 Normalization 的做法是 $\\tilde{\\boldsymbol{x}}_{i}^{r} \\leftarrow \\frac{\\boldsymbol{x}_{i}^{r}-\\mu_{i}}{\\sigma_{i}}$ ，其中 $\\mu_{i}$ 是均值，$\\sigma_{i}$ 是标准差 Batch Normalization 是指，对于一开始的 input 我们会做一次 Feature Normalization，当 input 经过一个 Network Layer 后，会得到 output，这个 output 其实可以看作是下一个 Layer 的 input，因此也需要做 Normalization。但会发现这一次的 Normalization 计算过程中需要的 $\\mu$ 和 $\\sigma$ 依赖于这一层所有的 output，而如果这一次训练数据过多，GPU Memory 是存不下这么多所需数据的，因此只考虑一个 batch 内的所有数据。另外在每一层 Normalization 后，通常会增加两个参数矩阵 $\\boldsymbol\\gamma$ 和 $\\boldsymbol\\beta$ ，默认为全 $1$ 和全 $0$：$$\\begin{aligned} &amp;\\boldsymbol{\\tilde{z}^{i}}=\\frac{\\boldsymbol{z^{i}}-\\mu}{\\sigma}\\\\ &amp;\\boldsymbol{\\hat{z}^{i}=\\gamma\\odot\\tilde{z}^{i}+\\beta}\\end{aligned}$$ 对于使用 Batch Normalization 后的模型，做 test 的时候是不可能等输入数据达到 train 时的 batch 大小的，这样的话 $\\mu$ 和 $\\sigma$ 如何计算？在 train 的过程中计算出 $\\mu$ 和 $\\sigma$ 的 moving average 当作 test 所需的 $\\mu$ 和 $\\sigma$：$$\\begin{aligned}&amp;\\overline{\\boldsymbol{\\mu}}\\leftarrow p\\overline{\\boldsymbol{\\mu}}+(1-p)\\boldsymbol{\\mu}^t\\\\ &amp;\\overline{\\boldsymbol{\\sigma}}\\leftarrow p\\overline{\\boldsymbol{\\sigma}}+(1-p)\\boldsymbol{\\sigma}^t \\text{, here } \\boldsymbol{\\sigma} \\text{ represents } \\boldsymbol{\\sigma^2}\\end{aligned}$$ 梯度下降的数理基础是一阶泰勒展开，假设当前有两个参数 $a$ 和 $b$ ，这俩参数将要修改为 $\\theta_1$ 和 $\\theta_2$ ，则新的 loss 值可以表示为 $$\\mathrm{L}\\big(\\theta\\big)\\approx\\mathrm{L}\\big(a,b\\big)+\\frac{\\partial\\mathrm{L}\\big(a,b\\big)}{\\partial\\theta_1}\\big(\\theta_1-a\\big)+\\frac{\\partial\\mathrm{L}\\big(a,b\\big)}{\\partial\\theta_2}\\big(\\theta_2-b\\big)$$ 只需要找这个被泰勒展开后的式子的最小值即可，但这个 $\\approx$ 成立的前提是你的移动值域范围够小，也就是 learning rate 要足够小 Backpropagation 核心原理是链式法则 Chain Rule，因为在使用 Gradient Descent 的时候，下式中对 $w$ 的偏导是不容易被直接计算的： $$L(\\theta)=\\sum_{n=1}^NC^n(\\theta)\\quad\\longrightarrow\\quad\\frac{\\partial L(\\theta)}{\\partial w}=\\sum_{n=1}^N\\frac{\\partial C^n(\\theta)}{\\partial w}$$ 通过使用链式法则，可以变形为如下： $$\\frac{\\partial C}{\\partial w}=\\frac{\\partial z}{\\partial w}\\frac{\\partial C}{\\partial z},\\quad\\frac{\\partial C}{\\partial z}=\\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a},\\quad\\frac{\\partial C}{\\partial a}=\\frac{\\partial z^{\\prime}}{\\partial a}\\frac{\\partial C}{\\partial z^{\\prime}}+\\frac{\\partial z^{\\prime\\prime}}{\\partial a}\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$$可以发现，这个求导的过程是递归的，因此可以建一个反图，从最后的 Output Layer 开始使用梯度优化参数 局部最小/最大，或者鞍点等，梯度都会为 0，此时 loss 值不会减小，通过对 loss 函数进行二次泰勒展开，可以发现通过二次微分项能判断是否是鞍点，也就是 Hession 矩阵，如果 H 矩阵的特征值全为正数，那么是局部最小；全为负数，则是局部最大；有正有负，则是鞍点 Batch Size 不是越大越好，也不是越小越好，如下图： Momentum 做法是将物理中小球滚下斜坡的思想引入，每次 Gradient Descent 的时候考虑上一步的方向，又或者说，当前移动的方向是现在到过去所有 Gradient 的加权和 Adam: RMSProp + Momentum Recurrent Neural NetworkCourses ML Lecture 21-1: Recurrent Neural Network (Part I) (youtube.com) ML Lecture 21-2: Recurrent Neural Network (Part II) (youtube.com) Notes RNN 简单来说就是原本 Neural Network 中的第 $i$ 层只由 $i-1$ 层的 output 作为 input，现在还接入了上一时刻 $t-1$ 加入这次的 input，形式化的说 $$\\mathbf{C}_{i}^{t}= \\mathcal{F}_{i}(\\mathbf{C}_{i}^{t-1}, \\space \\mathbf{Output}_{i-1}^{t})$$ 原始的 RNN 的 error surface 是十分崎岖的，它在一些部分非常平坦、一些部分非常陡峭，有一个解释是 RNN 中的一个参数 $w$ 会实际上累乘多次（因为 memory 的关系），因此一点点变动可能导致巨大变动，而有些则影响较小，如：$$1^{200}=1, \\quad 1.01^{200} \\approx 7.316, \\quad 10^{200}=1\\mathrm{e}{200}, \\quad 10.01^{200} \\approx 1.221\\mathrm{e}{200}$$ LSTM（Long Short-term Memory）是现在最普遍的 RNN 形式，它一定程度上解决了 RNN 的 error surface 过于崎岖的现象；具体来说，对于一个 Neural，会有三个 Gate 去控制它的 Input、Output、Forget 的程度，如下图 Convolutional Neural NetworkCourses 【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN) (youtube.com) Notes 图片通常是一个 3D-Tensor，包括长、宽、RGB三个色域通道共三个维度；如果直接把这个 3D-Tensor 拉伸变成一个 vector 输入给 Neural Network 的话，网络参数会非常庞大（说是这样会更容易 overfitting），但是哪怕是人脑对图片进行分析，也往往是注意一些局部 Pattern 的信息，因此可以考虑每个 Neuron 只关注图像中的一个 Pattern 通常对于 Pattern 的设定是，默认选择所有的 channel，只需要设置长宽和步长，即 kernel size = $3 \\times 3$ 和 stride = $2$ ，对于 stride 我们不希望设置太大，使各个 Pattern 之间有重叠，这样能保证图像中的每个部分的特征都会被看到；甚至来说，可以有多个 Neuron 的观察区域是完全一样的，以便充分提取 Pattern 的信息 以上是对于网络的第一个简化，第二个简化是，图像的局部特征往往跟位置没有关系，会出现有些神经元的参数完全一致，因此我们考虑将一些 Neuron 的参数进行共享，即 Parameters Sharing，常见的做法是，例如当前每个 Pattern 都有 64 个 Neuron 进行观察，那么每个区域的第一个 Neuron 相互共享参数、第二个 Neuron 相互共享参数……，分别称为 Filter 1, Filter 2, …. 以上内容换一个说法就是，现在有很多个滤波器 Filter，每个 Filter 会根据步长遍历完整个图像，一个 Filter 实质就是一组参数。 假设现在是彩色图像，那么初始有 3 个 channel，假设有 64 个 Filter，那么最后会得到 64 个滤波后的图像，也就是变成了一幅有 64 个 channel 的图像，如下图： Pooling 的主要动力是把图像变小，减小运算量，近几年随着算力提升，很多 Network 不再使用 Pooling，进行 Fully Convolution CNN 的 Pooling 不一定要使用，要根据任务特性来看（例如 Alpha GO），同样的，CNN 可以被用在语音识别、NLP 上，但是其中的 Receptive Field、Parameters Sharing 的设计都是不一样的：Speech、Natural Language Processing 普通的 CNN 无法处理图像的放缩、旋转的问题，通常使用 Data Augmentation，即把训练数据进行放缩、旋转进行数据扩充，也让 CNN 学习 Self-AttentionCourses 【機器學習2021】自注意力機制 (Self-attention) (上) (youtube.com) 【機器學習2021】自注意力機制 (Self-attention) (下) - YouTube Notes 单词表示成 vector 通常有两种做法：One-Hot Encoding、Word Embedding 语音识别上，通常取一个长度为 25ms 的 window，把语音信号提取除了作为一个 frame (also called vector in DL)，每次平移一个 10ms 的 stride Self-Attention 考虑整个句子的信息，经过 Self-Attention 之后再丢入 Fully Connected 考虑单个词汇的信息（这样哪怕是同一个词汇，在句子的不同位置也会不一样） 假设现在有一句子，提取出四个词汇向量 $\\boldsymbol a^{1…4}$，现在考虑如何计算 attention 值。首先定义 query、key、value 三个变换矩阵 $W^q$、$W^k$、$W^v$（它们是 network 的 parameters 通过 learn 得到），将 $\\boldsymbol a$ 变换成 $\\boldsymbol q$、$\\boldsymbol k$、 $\\boldsymbol v$：$$\\boldsymbol q = W^q \\boldsymbol a, \\quad \\boldsymbol k = W^k \\boldsymbol a, \\quad \\boldsymbol v = W^v \\boldsymbol a$$接着，定义 $\\alpha$ 表示向量间的相关程度，再经过激活函数（这里是 Softmax）计算为：$$\\alpha ^{1,1…4} = \\boldsymbol q^1 \\cdot \\boldsymbol k^{1…4}, \\quad\\alpha_{1,i}’=exp(\\alpha_{1,i})/\\sum_jexp(\\alpha_{1,j}), \\quad \\alpha_{2…4,i}’ = \\text{the same way}$$最后经过 Attention 后、带有整个句子信息的单个词汇 $\\boldsymbol b^{1…4}$ 为：$$\\boldsymbol b^1 = \\sum_{i}\\alpha_{1,i}’ \\boldsymbol v^i , \\quad\\boldsymbol b^{2…4} = \\sum_{i}\\alpha_{2…4,i}’ \\boldsymbol v^i$$ 将以上向量排列为矩阵，得到 $\\mathbf{Q}$、$\\mathbf{K}$、$\\mathbf{V}$ 如下：$$ \\mathbf{I}=\\boldsymbol{[a^1,…a^4]}, \\quad \\mathbf{Q}=\\boldsymbol{[q^1,…q^4]}, \\quad \\mathbf{K}=\\boldsymbol{[k^1,…k^4]}, \\quad \\mathbf{V}=\\boldsymbol{[v^1,…v^4]}$$之间的关系为：$$\\mathbf{Q}=W^q\\mathbf{I},\\quad\\mathbf{K}=W^k\\mathbf{I},\\quad\\mathbf{V}=W^v\\mathbf{I}$$而相关程度 $\\alpha$ 的矩阵形式为：$$A=\\begin{bmatrix} \\alpha_{1,1}&amp; \\alpha_{2,1}&amp; \\alpha_{3,1}&amp; \\alpha_{4,1} \\\\ \\alpha_{1,2}&amp; \\alpha_{2,2}&amp; \\alpha_{3,2}&amp; \\alpha_{4,2} \\\\ \\alpha_{1,3}&amp; \\alpha_{2,3}&amp; \\alpha_{3,3}&amp; \\alpha_{4,3} \\\\ \\alpha_{1,4}&amp; \\alpha_{2,4}&amp; \\alpha_{3,4}&amp; \\alpha_{4,4}\\end{bmatrix}= \\mathbf{K}^T \\cdot \\mathbf{Q}, \\quad A’=\\mathcal{Softmax}(A)$$最后的输出为：$$\\mathbf{O} = \\mathbf{V} \\cdot A’$$ Multi-Head Attention 认为向量间有多个相关性，需要用多个 Attention 来评估。相较 Self-Attention 来说，如果你需要 $n$ 个 head，那么就把 $\\boldsymbol q$、$\\boldsymbol k$、$\\boldsymbol v$ 乘上 $n$ 个变换矩阵得到多个 $\\boldsymbol q$、$\\boldsymbol k$、$\\boldsymbol v$，然后对应的 $\\boldsymbol q$、$\\boldsymbol k$、$\\boldsymbol v$ 之间进行后续同样的操作，最后再把多个 $\\boldsymbol b$ 使用一个合并的矩阵合并即可 由于 Self-Attention 本身对于任意一个向量进行的操作都是一样的，它其实不具备位置信息，但有的时候位置的信息会对结果有很大影响。这里就需要 Position Encoding，可以让输入 $\\boldsymbol {a^i + e^i}$，这个 $\\boldsymbol e^i$ 是一个人为构造的位置向量，在 Transformer 中是一个 sin &amp; cos 的函数，实际上也可以是别的形式，目前也有相关 paper 在研究 Self-Attention 用于 Speech 时，由于 vector 众多，有一种 Truncated Self-Attention，做法就是不考虑整个 set，只考虑 $[i-j,i+j]$ 的部分 Self-Attention 本身的输入要求是一个 vector set，它其实可以用在图像中，例如一个 $5 \\times 10$ RGB 的图像，我们可以将一个 pixel 的三个 channel 看成一个向量，就是说图像可以被认为是有 50 个三维向量构成。CNN 可以说是 Self-Attention 的子集，也有 paper 说明，当 Self-Attention 被设置为某种情况下时，Self-Attention 就是 CNN 在图神经网络（Graph Neural Network, GNN）中，Self-Attention 也可以被使用。由于给出的 Graph 往往时 Domain Expert 建立的 Domain Knowledge Graph，因此在计算 Attention Matrix 的时候只需要计算 existed-edge 的点对 给出了 Self-Attention 的一些变形：Efficient Transformers: A Survey、Long Range Arena: A Benchmark for Efficient Transformers TransformerCourses 【機器學習2021】Transformer (上) (youtube.com) 【機器學習2021】Transformer (下) (youtube.com) Notes Sequence to Sequence 的模型可以被用于解决很多问题，许多看似不可行的问题（比如句子语法树的构建）甚至可以硬解，Seq2Seq 的结构是 Input Seq $\\rightarrow$ Encoder $\\rightarrow$ Decoder $\\rightarrow$ Output Seq，输入输出的长度不要求对应 Multi-Label Classification 和 Multi-Class Classification 的区别是，第一个是每个 example 要输出一个或多个 class，第二个是现在的 class 集合庞大，每个 example 需要从中选择一个 class Residual Connection 是指一种将 input 和 output 相加得到新的 output 的方式 Transformer 中运用的 normalization 是 layer normalization 而非 batch normalization，二者的区别在于 batch 是对多个 example 的同一维度进行标准化，而 layer 是对一个 example 的多个维度进行标准化，可参考博客 Masked Multi-Attention 和不加 mask 的区别在于，例如，之前计算 $b_i$ 的时候需要看$a_{1…n}$，但现在只看 $a_{1…i}$，就是说取 $q^i$ 和 $k^{1…i}$、$v^{1…i}$ 进行计算 在 Decoder 和 Encoder 之间的部分，图例可以看到 Encoder 提供两个箭头、Decoder 提供一个箭头进行 Attention 的计算。实际的意思是，取 Encoder 的 output 进行 transform 得到 $k$ 和 $v$，然后取 Decoder 的 input 进行 transform 得到 $q$，这个过程叫 Cross Attention Copy Mechanism 是说，在 chatbot、paper summary 这类应用中，有些名词句子并不需要独立由模型生成，因为有些词汇是有特殊意义的、专业名词等，在 training 过程中并没有出现，这个时候模型就需要直接进行原文的引用 Guided Attention 是指人为的限定哪些 input 的 attention score 应该更高，output 应该更关注这些 Beam Search 是指在开始放弃一些更优 score 的 output（通常都是一直取最大 score 的，称为 Greedy Decoding），可能会使最后结果更好。但这个做法并不一直 work，Hungyi Lee 认为可能在一些需要 creative 的 task 中不会使结果更好，而对于一些 output 在一个集合内、输出内容范围清晰的 task 中可能更好一点 对于语音辨识系统，training 使用的是 Cross Entropy 而真正评估的指标是 BLEU score，BLEU score 是求不出 gradient 的，因此无法进行 gradient descent，但是可以考虑使用 Reinforcement Learning 硬 train 一发 上面提到 Decoder 在 train 时会有真正正确的 label 作为 input，但是在实际使用 model 的时候所读取作为 input 的东西是 model 对之前的部分自行 predict 的，训练时这之前的部分是一定正确的，而此时不一定正确，但由于 model 被训为认为之前的内容一定正确，所以 predict 会出现 mismatch 并一错就接二连三的错。一种解决方法是在训练的时候加入一定的 noise，这种做法叫 Scheduled Sampling Generative ModelCourses 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹 (youtube.com) 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGAN (youtube.com) 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (三) – 生成器效能評估與條件式生成 (youtube.com) Notes 平常的 Network 加一个 Simple Distribution 从而得到 Complex Distribution 的过程，就是 Generator。Generative model 通常是人们需要它具有一定创造性的时候 除了需要训练一个 Generator 之外，还需要训练一个 Discriminator 用来判断（例如）图片是不是二次元人物 Generator v1 首先随机生成一些图片，然后 Discriminator v1 会从中选择最像二次元人物的保留；然后为了骗过 Discriminator v1，Generator v2 会基本完全满足 Discriminator v1 的筛选条件，而 Discriminator v2 会以更加细致、严格的标准去筛选……如此迭代。一个说法是这好像蝴蝶和它的天敌在几百万年的自然选择、一个说法是造假钞的和辨别真假钞的警察，看起来 Generator 和 Discriminator 好像在对抗斗争一样，因此使用了 Adversarial(对抗性) 这个单词 GAN 的训练步骤有两步，反复这两步 第一步固定 Generator，用真实的二次元人物和 Generator 产生的图像作为 Discriminator 的输入，用 Regression 或者 Classification 的方法训练 Discriminator 第二部固定 Discriminator，先使用 Simple Distribution 的 Sample 作为输入给 Generator，产生的图片经过 Discriminator 后得到对应的分数，然后调整 Generator 的参数使得产生的图片可以获得 Discriminator 更高的评估 Network Structure: Input $\\rightarrow$ Generator $\\rightarrow$ Image $\\rightarrow$ Discriminator $\\rightarrow$ Evaluation(Output) 考虑整体来看，Generator 和 Discriminator 相当于一个 Network 的前几层和后几层，先 Fix 前半部分训练后半，再 Fix 后半部分训练前半 我们希望 $P_G$ (Generated) 和 $P_{data}$ 之间的 divergence 越小越好，但问题在于这个 divergence 不好计算。考虑对 $P_G$ 和 $P_{data}$ 分别进行 sample，G 的 sample 是指，从 simple distribution 中进行 sample 给 Generator 生成图片；然后使用 Discriminator 对两边算出分数，而 Discriminator 需要对真图给高分，给生成的图低分。我们定义 Objective Function：$$V(G,D)=E_{y\\sim P_{data}}[logD(y)]+E_{y\\sim P_{G}}[log(1-D(y))]$$我们希望其越大越好（希望越小越好的一般称为 Loss Function），具体来说，我们希望 $logD(y)$ 对于 $P_{data}$ 越大越好，对于 $P_G$ 越小越好。事实上，这个 Objective Function 是 Cross Entropy 加上负号，那么 optimization 就是 train 一个 Binary Classifier 了。我们希望的 Training 目标如下：$$D^{}=arg\\max_{D}V(D,G)$$回到一开始，我们希望 divergence 越小越好，GAN 原始 paper 进行了一些推导，发现 $\\max_{D}V(D,G)$ 与 JS Divergence 相关，直观上可以理解，当两类直接的 divergence 很小，Binary Classifier 是比较难分辨的，也就是 $D(y)$ 对于 $P_G$ 和 $P_{data}$ 来说都很接近；而当 divergence 很大时，$D(y)$ 对于 $P_G$ 和 $P_{data}$ 差异会变大。得到这样的推导后，我们可以把 $Div()$ 替换成 $D^$，即$$G^*=arg\\min_GDiv(P_G,P_{data})=arg\\min_G\\max_{D}V(D,G)$$这代表我们希望在 G 固定的情况下去找到一个 D，然后在 D 固定的情况下去找到一个 G，如此往复，也就和之前说的是一个意思了。当然，Objective Function 或者是 Divergence 可以有别的，这里有一篇 paper JS Divergence 在两部分没有一点重叠的时候始终是常数 $\\log2$ ，梯度消失，无法 update，但是实际上可能两个不重叠部分已经越来越接近了，解决办法之一是 Wasserstein Distance 使用 Wasserstein Distance 的 GAN 叫 WGAN，Wassertein Distance 具体怎么计算这里不讨论，只给出结果：$$\\max_{D\\in1-Lipschitz}\\left{E_{y\\sim P_{data}}[D(y)]-E_{y\\sim P_{G}}[D(y)]\\right}$$其中 $1-Lipschitz$ 是 Lipschitz 连续的一种：$$|f(x_1)-f(x_2)|\\leq K\\cdot|x_1-x_2|$$当 $K=1$ 时，表示 $1-Lipschitz$ 连续。意思是 Discriminator 得是一个始终满足以上条件的函数，而为了满足这个条件，有许多方法。第一种是 Original WGAN，强制让所有 parameters 在 $[-c,c]$ 之间，如果某个参数太小/太大则将其修改为 $-c/c$ ；另外有 Improved WGAN、Spectral Normalization","link":"/2024/07/22/Machine-Learning-Notes/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Study","slug":"Study","link":"/tags/Study/"},{"name":"Contest","slug":"Contest","link":"/tags/Contest/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"}],"categories":[{"name":"Machine Learning Course","slug":"Machine-Learning-Course","link":"/categories/Machine-Learning-Course/"},{"name":"XCPC","slug":"XCPC","link":"/categories/XCPC/"},{"name":"Codeforces","slug":"XCPC/Codeforces","link":"/categories/XCPC/Codeforces/"}],"pages":[]}