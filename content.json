{"posts":[{"title":"Machine Learning Notes","text":"Course Links: 【生成式AI】快速了解機器學習基本原理 (1/2) (已經略懂機器學習的同學可以跳過這段) - YouTube ML Lecture 6: Brief Introduction of Deep Learning - YouTube ML Lecture 3-1: Gradient Descent - YouTube ML Lecture 7: Backpropagation (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point) (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum) (youtube.com) ML or DL 都可以理解为三个步骤： Define a function set: Deep Learning(CNN, Transformer), Decision Tree, etc. Goodness of function: Supervised Learning, Semi-supervised Learning, Reinforced Learning, etc. Pick the best function: Gradient Descent(Adam, AdamW), Genetic Algorithm, etc. Gradient Descent 就是用总的 Loss 对网络中每个权重和偏置求偏导，然后用这个参数减去即可，即 $w_{1}-\\mu \\frac{\\partial L}{\\partial w_{1}}$ ，其中 $\\mu$ 表示 Learning Rate，通常来说，$\\eta$ 要么为一开始的固定值，要么为一个关于 $t$ 的函数，并且通常随 $t$ 增大而变小 训练模型时，最后把 epochs / loss 的折线图输出，方便观察当前 learning rate 下参数变化导致的 loss 变化趋势，以便调整 learning rate Learning Rate 不仅跟一阶微分成正比，还和二阶微分成反比，即 $\\frac{ \\left| First \\space Derivative \\right|}{Second \\space Derivative}$ Adaptive Learning Rates 就是说针对每个参数都给一个独立的 Learning Rate，比如 Adagrad，将原始的参数更新式修改为 $w_{t}-\\frac{\\eta^{t}}{\\sigma^{t}}g^{t}$ ，其中 $\\sigma^t$ : root mean square of the previous derivatives of parameter $w$ ，$\\eta^{t}$ : time $t$’s learning rate，$\\sigma^t=\\sqrt{\\frac{1}{t+1}\\sum_{i=0}^{t}(g^{i})^{2}}$ ，Adagrad 反映了每次的梯度的反差大小 Stochastic Gradient Descent 简单来说就是不求所有 sample 的 loss 和，随机选择一个 sample 计算它的 loss 和 gradient 作为梯度下降的依据，更新所有参数；传统梯度下降会把 $n$ samples 看一遍然后更新一次，这种做法看完 $n$ samples 后总共会更新 $n$ 次 对于数据做 Feature Scaling(Normalization) 是有必要的，这可以使训练时的 learning rate 相对统一（每一种数据对参数的影响相当），通常 Normalization 的做法时 $x_i^r\\leftarrow\\frac{x_i^r-m_i}{\\sigma_i}$ ，其中 $m_{i}$ 时均值，$\\sigma_{i}$ 是标准差 梯度下降的数理基础是一阶泰勒展开，假设当前有两个参数 $a$ 和 $b$ ，这俩参数将要修改为 $\\theta_1$ 和 $\\theta_2$ ，则新的 loss 值可以表示为 $$\\mathrm{L}\\big(\\theta\\big)\\approx\\mathrm{L}\\big(a,b\\big)+\\frac{\\partial\\mathrm{L}\\big(a,b\\big)}{\\partial\\theta_1}\\big(\\theta_1-a\\big)+\\frac{\\partial\\mathrm{L}\\big(a,b\\big)}{\\partial\\theta_2}\\big(\\theta_2-b\\big)$$ 只需要找这个被泰勒展开后的式子的最小值即可，但这个 $\\approx$ 成立的前提是你的移动值域范围够小，也就是 learning rate 要足够小 Backpropagation 核心原理是链式法则 Chain Rule，因为在使用 Gradient Descent 的时候，下式中对 $w$ 的偏导是不容易被直接计算的： $$L(\\theta)=\\sum_{n=1}^NC^n(\\theta)\\quad\\longrightarrow\\quad\\frac{\\partial L(\\theta)}{\\partial w}=\\sum_{n=1}^N\\frac{\\partial C^n(\\theta)}{\\partial w}$$ 通过使用链式法则，可以变形为如下： $$\\frac{\\partial C}{\\partial w}=\\frac{\\partial z}{\\partial w}\\frac{\\partial C}{\\partial z},\\quad\\frac{\\partial C}{\\partial z}=\\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a},\\quad\\frac{\\partial C}{\\partial a}=\\frac{\\partial z^{\\prime}}{\\partial a}\\frac{\\partial C}{\\partial z^{\\prime}}+\\frac{\\partial z^{\\prime\\prime}}{\\partial a}\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$$可以发现，这个求导的过程是递归的，因此可以建一个反图，从最后的 Output Layer 开始使用梯度优化参数 局部最小/最大，或者鞍点等，梯度都会为 0，此时 loss 值不会减小，通过对 loss 函数进行二次泰勒展开，可以发现通过二次微分项能判断是否是鞍点，也就是 Hession 矩阵，如果 H 矩阵的特征值全为正数，那么是局部最小；全为负数，则是局部最大；有正有负，则是鞍点 Batch Size 不是越大越好，也不是越小越好，如下图： Momentum 做法是将物理中小球滚下斜坡的思想引入，每次 Gradient Descent 的时候考虑上一步的方向，又或者说，当前移动的方向是现在到过去所有 Gradient 的加权和 Adam: RMSProp + Momentum","link":"/2024/07/22/Machine-Learning-Notes/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/04/30/hello-world/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"}],"categories":[{"name":"Machine Learning Course","slug":"Machine-Learning-Course","link":"/categories/Machine-Learning-Course/"}],"pages":[]}