{"posts":[{"title":"ML2023Spring-hw1","text":"Regression TaskLink: https://www.kaggle.com/t/a339b77fa5214978bfb8dde62d3151fe 本次 homework 的任务是根据 COVID-19 Cases 进行 Prediction，主要使用了线性全连接层和 ReLU 激活函数构建网络。 为了达到更高的 Baseline 采用了特征截取、Adam 学习器、L2 正则化（weight decay in optimizer）并在 Sample 代码基础上增加了网络深度 Network StructureBrief Intro1 input layer (implicit) 3 linear layers (with ReLU activation function) 1 output layer Core Code123456789self.layer1 = nn.Sequential( nn.Linear(inputs, 16), nn.ReLU(), nn.Linear(16, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 1)) Learned Things这篇笔记是事后补的，记忆有些凌乱，以下记录不具有任何逻辑顺序 事先定义一个设置随机数种子的函数以确保每次运行结果一致 有些用于评估的变量记得添加 detach 使其从计算图中分离 数据读入三步走：划分训练集和验证集、将 data 和 label 通过 Dataset 打包、通过 DataLoader 将 dataset 创建维 loader 供训练使用 epochs 较多时可以采用早停法提高训练效率 数据提前进行特征数据筛选/截取，将明显无关的数据扔掉 Full CodePrivate Score: 0.84523 Public score: 0.82729 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210# Numerical Operationsimport mathimport numpy as np# Reading/Writing Dataimport pandas as pdimport osimport csv# For Progress Barfrom tqdm import tqdm# Pytorchimport torch import torch.nn as nnfrom torch.utils.data import Dataset, DataLoader, random_split# For plotting learning curvefrom torch.utils.tensorboard import SummaryWriterclass MyDataset(Dataset): def __init__(self, x, y=None): if y is not None: self.y = torch.tensor(y, dtype=torch.float32) self.x = torch.tensor(x, dtype=torch.float32) def __getitem__(self, index): if hasattr(self, 'y'): return self.x[index], self.y[index] else: return self.x[index] def __len__(self): return len(self.x) class LinearModel(nn.Module): def __init__(self, inputs): super(LinearModel, self).__init__() self.layer1 = nn.Sequential( nn.Linear(inputs, 16), nn.ReLU(), nn.Linear(16, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 1) ) def forward(self, x): x = self.layer1(x) x = x.squeeze(1) # (X, 1) -&gt; (X) return xdef set_seed(seed): np.random.seed(seed) torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)def split_data(dataset, valid_rate, seed): valid_size = int(valid_rate * len(dataset)) train_size = len(dataset) - valid_size train_set, valid_set = random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(seed)) return np.array(train_set), np.array(valid_set)def predict(test_loader, model, device): model.eval() preds = [] for x in tqdm(test_loader): x = x.to(device) with torch.no_grad(): pred = model(x) preds.append(pred.detach().cpu()) # detach 的作用是将张量从这个计算图中分离出来， # 使其不再跟踪计算图中的操作 preds = torch.cat(preds, dim=0).numpy() return predsdef select_feat(train_data, valid_data, test_data, select_all=True): y_train, y_valid = train_data[:,-1], valid_data[:,-1] raw_x_train, raw_x_valid, raw_x_test = train_data[:, :-1], valid_data[:, :-1], test_data if select_all: feat_idx = list(range(raw_x_train.shape[1])) else: feat_idx = list(range(35, raw_x_train.shape[1])) return raw_x_train[:, feat_idx], raw_x_valid[:, feat_idx], raw_x_test[:, feat_idx], y_train, y_validdef train_model(train_loader, valid_loader, model, args, device): critertion = nn.MSELoss(reduction='mean') optimizer = torch.optim.Adam(model.parameters(), lr=args['learning_rate'], weight_decay=1e-4) writer = SummaryWriter() if not os.path.exists('./models'): os.mkdir('./models') epochs, best_loss, step, early_stop = args['epochs'], math.inf, 0, 0 for i in range(epochs): model.train() loss_sum = [] tarin_process_bar = tqdm(train_loader, position=0, leave=True) for x, y in tarin_process_bar: optimizer.zero_grad() x, y = x.to(device), y.to(device) pred = model(x) loss = critertion(pred, y) loss.backward() optimizer.step() step += 1 loss_sum.append(loss.detach().item()) tarin_process_bar.set_description(f'Epoch [{i+1}/{epochs}]') tarin_process_bar.set_postfix({'loss': loss.detach().item()}) mean_train_loss = sum(loss_sum) / len(loss_sum) writer.add_scalar('loss/train', mean_train_loss, step) model.eval() loss_sum = [] for x, y in valid_loader: x, y = x.to(device), y.to(device) with torch.no_grad(): pred = model(x) loss = critertion(pred, y) loss_sum.append(loss.item()) mean_valid_loss = sum(loss_sum) / len(loss_sum) print(f'Epoch [{i+1}/{epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}') writer.add_scalar('loss/valid', mean_valid_loss, step) if mean_valid_loss &lt; best_loss: best_loss = mean_valid_loss torch.save(model.state_dict(), args['save_path']) print(f'Saving model with loss {best_loss:.3f}') early_stop = 0 else: early_stop += 1 if (early_stop == args['early_stop']): print('\\nModel is not improving, so we stop training') return def save_pred(preds, file): ''' Save predictions to specified file ''' with open(file, 'w', newline='') as fp: writer = csv.writer(fp) writer.writerow(['id', 'tested_positive']) for i, p in enumerate(preds): writer.writerow([i, p])def main(args): device = 'cuda' if torch.cuda.is_available() else 'cpu' set_seed(args['seed']) # train_data size: 3009 x 89 # test_data size: 997 x 88 train_data, test_data = pd.read_csv('./covid_train.csv').values, pd.read_csv('./covid_test.csv').values train_data, valid_data = split_data(train_data, args['valid_ratio'], args['seed']) print(f'train_data size: {train_data.shape}\\nvalid_data size: {valid_data.shape}\\ntest_data size: {test_data.shape}') x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, args['select_all']) train_dataset, valid_dataset, test_dataset = MyDataset(x_train, y_train), \\ MyDataset(x_valid, y_valid), \\ MyDataset(x_test) # Pytorch data loader loads pytorch dataset into batches. train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, pin_memory=True) valid_loader = DataLoader(valid_dataset, batch_size=args['batch_size'], shuffle=True, pin_memory=True) test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False, pin_memory=True) # print(train_loader) model = LinearModel(inputs=x_train.shape[1]).to(device) train_model(train_loader, valid_loader, model, args, device) model.load_state_dict(torch.load(args['save_path'])) output = predict(test_loader, model, device) print(output) save_pred(output, 'pred.csv') if __name__ == '__main__': args = { 'seed': 42, 'select_all': False, 'valid_ratio': 0.2, 'epochs': 3000, 'batch_size': 128, 'learning_rate': 1e-3, 'early_stop': 600, 'save_path': './models/model.ckpt' } main(args)","link":"/2024/07/24/ML2023Spring-hw1/"},{"title":"Machine Learning Notes","text":"Course Links: 【生成式AI】快速了解機器學習基本原理 (1/2) (已經略懂機器學習的同學可以跳過這段) - YouTube ML Lecture 6: Brief Introduction of Deep Learning - YouTube ML Lecture 3-1: Gradient Descent - YouTube ML Lecture 7: Backpropagation (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point) (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum) (youtube.com) ML or DL 都可以理解为三个步骤： Define a function set: Deep Learning(CNN, Transformer), Decision Tree, etc. Goodness of function: Supervised Learning, Semi-supervised Learning, Reinforced Learning, etc. Pick the best function: Gradient Descent(Adam, AdamW), Genetic Algorithm, etc. Gradient Descent 就是用总的 Loss 对网络中每个权重和偏置求偏导，然后用这个参数减去即可，即 $w_{1}-\\mu \\frac{\\partial L}{\\partial w_{1}}$ ，其中 $\\mu$ 表示 Learning Rate，通常来说，$\\eta$ 要么为一开始的固定值，要么为一个关于 $t$ 的函数，并且通常随 $t$ 增大而变小 训练模型时，最后把 epochs / loss 的折线图输出，方便观察当前 learning rate 下参数变化导致的 loss 变化趋势，以便调整 learning rate Learning Rate 不仅跟一阶微分成正比，还和二阶微分成反比，即 $\\frac{ \\left| First \\space Derivative \\right|}{Second \\space Derivative}$ Adaptive Learning Rates 就是说针对每个参数都给一个独立的 Learning Rate，比如 Adagrad，将原始的参数更新式修改为 $w_{t}-\\frac{\\eta^{t}}{\\sigma^{t}}g^{t}$ ，其中 $\\sigma^t$ : root mean square of the previous derivatives of parameter $w$ ，$\\eta^{t}$ : time $t$’s learning rate，$\\sigma^t=\\sqrt{\\frac{1}{t+1}\\sum_{i=0}^{t}(g^{i})^{2}}$ ，Adagrad 反映了每次的梯度的反差大小 Stochastic Gradient Descent 简单来说就是不求所有 sample 的 loss 和，随机选择一个 sample 计算它的 loss 和 gradient 作为梯度下降的依据，更新所有参数；传统梯度下降会把 $n$ samples 看一遍然后更新一次，这种做法看完 $n$ samples 后总共会更新 $n$ 次 对于数据做 Feature Scaling(Normalization) 是有必要的，这可以使训练时的 learning rate 相对统一（每一种数据对参数的影响相当），通常 Normalization 的做法时 $x_i^r\\leftarrow\\frac{x_i^r-m_i}{\\sigma_i}$ ，其中 $m_{i}$ 时均值，$\\sigma_{i}$ 是标准差 梯度下降的数理基础是一阶泰勒展开，假设当前有两个参数 $a$ 和 $b$ ，这俩参数将要修改为 $\\theta_1$ 和 $\\theta_2$ ，则新的 loss 值可以表示为 $$\\mathrm{L}\\big(\\theta\\big)\\approx\\mathrm{L}\\big(a,b\\big)+\\frac{\\partial\\mathrm{L}\\big(a,b\\big)}{\\partial\\theta_1}\\big(\\theta_1-a\\big)+\\frac{\\partial\\mathrm{L}\\big(a,b\\big)}{\\partial\\theta_2}\\big(\\theta_2-b\\big)$$ 只需要找这个被泰勒展开后的式子的最小值即可，但这个 $\\approx$ 成立的前提是你的移动值域范围够小，也就是 learning rate 要足够小 Backpropagation 核心原理是链式法则 Chain Rule，因为在使用 Gradient Descent 的时候，下式中对 $w$ 的偏导是不容易被直接计算的： $$L(\\theta)=\\sum_{n=1}^NC^n(\\theta)\\quad\\longrightarrow\\quad\\frac{\\partial L(\\theta)}{\\partial w}=\\sum_{n=1}^N\\frac{\\partial C^n(\\theta)}{\\partial w}$$ 通过使用链式法则，可以变形为如下： $$\\frac{\\partial C}{\\partial w}=\\frac{\\partial z}{\\partial w}\\frac{\\partial C}{\\partial z},\\quad\\frac{\\partial C}{\\partial z}=\\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a},\\quad\\frac{\\partial C}{\\partial a}=\\frac{\\partial z^{\\prime}}{\\partial a}\\frac{\\partial C}{\\partial z^{\\prime}}+\\frac{\\partial z^{\\prime\\prime}}{\\partial a}\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$$可以发现，这个求导的过程是递归的，因此可以建一个反图，从最后的 Output Layer 开始使用梯度优化参数 局部最小/最大，或者鞍点等，梯度都会为 0，此时 loss 值不会减小，通过对 loss 函数进行二次泰勒展开，可以发现通过二次微分项能判断是否是鞍点，也就是 Hession 矩阵，如果 H 矩阵的特征值全为正数，那么是局部最小；全为负数，则是局部最大；有正有负，则是鞍点 Batch Size 不是越大越好，也不是越小越好，如下图： Momentum 做法是将物理中小球滚下斜坡的思想引入，每次 Gradient Descent 的时候考虑上一步的方向，又或者说，当前移动的方向是现在到过去所有 Gradient 的加权和 Adam: RMSProp + Momentum","link":"/2024/07/22/Machine-Learning-Notes/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/04/30/hello-world/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"}],"categories":[{"name":"Machine Learning Course","slug":"Machine-Learning-Course","link":"/categories/Machine-Learning-Course/"}],"pages":[]}