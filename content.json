{"posts":[{"title":"ML2023Spring-hw2","text":"Classification TaskLink: ML2023Spring-hw2 | Kaggle 本次 homework 的任务是根据音频文件，对每一个 phoneme 进行多分类任务，一共有 41 个类别，所有的数据预处理由 TA 给出，此处先不做任何学习、考虑。使用了 MLP 多层感知机作为网络主体 为了达到更高的 Baseline 采用了 BatchNorm，并在 Sample 代码基础上增加了网络深度、提高了 concat_nframes 的数值 Network StructureBrief Intro1 input layer (implicit) 6 linear layers (with ReLU, BatchNorm1d) 1 output layer Core Code123456789self.layer1 = nn.Sequential( nn.Linear(inputs, 16), nn.ReLU(), nn.Linear(16, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 1)) Learned Things这篇笔记是事后补的，记忆有些凌乱，以下记录不具有任何逻辑顺序 事先定义一个设置随机数种子的函数以确保每次运行结果一致 有些用于评估的变量记得添加 detach 使其从计算图中分离 数据读入三步走：划分训练集和验证集、将 data 和 label 通过 Dataset 打包、通过 DataLoader 将 dataset 创建维 loader 供训练使用 epochs 较多时可以采用早停法提高训练效率 数据提前进行特征数据筛选/截取，将明显无关的数据扔掉 Full CodePrivate Score: 0.84523 Public score: 0.82729","link":"/2024/07/24/ML2023Spring-hw2/"},{"title":"Codeforces Round 961 (Div. 2)","text":"太久没写题了，脑力猛猛下降，后面题其实能做，但这段时间主力在学 Machine Learning， 有时间再补吧，去看李宏毅去了（逃 A. Diagonals题意$n \\times n$ 的棋盘上放 $k$ 个棋子，左下到右上共 $n$ 条对角线，问所有放置方案中最少有几条对角线被覆盖 题解简单模拟，写的时候脑子瘫痪了，第一反应没注意最后剩的不完整的一条对角线也算 代码1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;set&gt;#include &lt;queue&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;#define N (10010)#define inf (0x3f3f3f3f)#define INF (0x3f3f3f3f3f3f3f3fll)#define LL long longint T, n, k;inline int read() { int x = 0, f = 1; char ch = getchar(); while (ch &lt; '0' || ch &gt; '9') ch == '-' ? f = -1, ch = getchar() : ch = getchar(); while (ch &gt;= '0' &amp;&amp; ch &lt;= '9') x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (ch ^ 48), ch = getchar(); return x * f;}int main() { ios::sync_with_stdio(false); T = read(); while (T--) { n = read(), k = read(); int ans = 0; for (int i = n; i &gt;= 1 &amp;&amp; k; --i) { if (k &gt; i) k -= i, ++ans; else { ++ans; break; } if (i != n) { if (k &gt; i) k -= i, ++ans; else { ++ans; break; } } } printf(&quot;%d\\n&quot;, ans); } return 0;} B1. Bouquet (Easy Version)题意有 $n$ 朵花，每一朵花有 $a_i$ 个花瓣，购买它需要花费 $a_i$ 元，现在有 $m$ 元，要求购买的花里面，任意两朵之间的花瓣数差距不超过 $1$，问最多的花瓣数是多少 题解由于题意限制，可以认为只能购买 $x$ 和 $x+1$ 两种花瓣数的花朵，因此先对花瓣数相同的进行统计，假设购买 $k_1$ 朵 $x$ 的花，那么可以再购买 $\\left \\lfloor \\frac{m-k_1 \\times x}{k+1} \\right \\rfloor$ 朵 $x+1$ 的花，最后获得的总花瓣数可以计算得到 注意这道题在写的时候发现 unordered_map 的遍历始终有问题，本地 C++ 17 提交 C++ 14 运行后结果与本地不一致，后将 Codeforces 语言也设置为 C++ 17 后挂在了第 9 个点，随后改成 map 后再提交通过，狠狠 WA 了几发目前问题暂不明确，网上没有搜到相关信息 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;set&gt;#include &lt;queue&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;map&gt;using namespace std;#define N (200010)#define inf (0x3f3f3f3f)#define INF (0x3f3f3f3f3f3f3f3fll)#define int long longint T, n, m;map&lt;int, int&gt; a;inline int read() { int x = 0, f = 1; char ch = getchar(); while (ch &lt; '0' || ch &gt; '9') ch == '-' ? f = -1, ch = getchar() : ch = getchar(); while (ch &gt;= '0' &amp;&amp; ch &lt;= '9') x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (ch ^ 48), ch = getchar(); return x * f;}signed main() { ios::sync_with_stdio(false); T = read(); while (T--) { n = read(), m = read(); int tmp; for (int i = 1; i &lt;= n; ++i) tmp = read(), a[tmp]++; int Max = 0; for (auto x = a.begin(); x != a.end(); ++x) { int t = (*x).first; for (int i = 1; i &lt;= a[t]; ++i) { if (i * t &gt; m) continue; int res = m - i * t, j = 0; if (a[t + 1] != 0) j = min(a[t + 1], res / (t + 1)); Max = max(Max, i * t + j * (t + 1)); } } a.clear(); printf(&quot;%lld\\n&quot;, Max); } return 0;} B2. Bouquet (Hard Version)题意如上，但范围变大 题解不难发现，剩的钱越少，答案越大。如果先尽可能拿 $x$ 的花，然后再尽可能的拿 $x + 1$ 的花，如果还剩钱，就从剩的钱里面取 $1$ 元、少拿一个 $x$ 的花，这样就又有一个 $x + 1$ 的花了，这样的贪心策略显然可以使得剩的钱最少 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;set&gt;#include &lt;queue&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;#define N (200010)#define inf (0x3f3f3f3f)#define INF (0x3f3f3f3f3f3f3f3fll)#define int long longint T, n, m;struct Flower { int a, c; friend bool operator&lt; (const Flower &amp;a, const Flower &amp;b) { return a.a &lt; b.a; }} F[N];inline int read() { int x = 0, f = 1; char ch = getchar(); while (ch &lt; '0' || ch &gt; '9') ch == '-' ? f = -1, ch = getchar() : ch = getchar(); while (ch &gt;= '0' &amp;&amp; ch &lt;= '9') x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (ch ^ 48), ch = getchar(); return x * f;}signed main() { ios::sync_with_stdio(false); T = read(); while (T--) { n = read(), m = read(); for (int i = 1; i &lt;= n; ++i) F[i].a = read(); for (int i = 1; i &lt;= n; ++i) F[i].c = read(); sort(F + 1, F + n + 1); int ans = 0; for (int i = 1; i &lt;= n; ++i) { ans = max(ans, min(F[i].c, m / F[i].a) * F[i].a); int sum = 0, cnt1 = 0, cnt2 = 0, res; if (i != n &amp;&amp; F[i].a + 1 == F[i + 1].a) { // i != n is must! cnt1 = min(F[i].c, m / F[i].a); res = m - cnt1 * F[i].a; cnt2 = min(F[i + 1].c, res / F[i + 1].a); res = res - cnt2 * F[i + 1].a; sum = cnt1 * F[i].a + cnt2 * F[i + 1].a; sum += min(min(res, cnt1), F[i + 1].c - cnt2); ans = max(ans, sum); } } printf(&quot;%lld\\n&quot;, ans); } return 0;}","link":"/2024/07/25/Codeforces-Round-961-Div-2/"},{"title":"Machine Learning Notes","text":"主要记录个人学习李宏毅的 Machine Learning 课程的笔记，其实更像是随笔，写的比较随意，会写一些概念理论，但更多会写令自己豁然开朗的部分。另外本篇文章不时更新，如果本篇内容有帮助到你，我会感到非常荣幸！ Deep LearningCourses 【生成式AI】快速了解機器學習基本原理 (1/2) (已經略懂機器學習的同學可以跳過這段) - YouTube 【生成式AI】快速了解機器學習基本原理 (2/2) (已經略懂機器學習的同學可以跳過這段) (youtube.com) ML Lecture 6: Brief Introduction of Deep Learning - YouTube ML Lecture 3-1: Gradient Descent - YouTube ML Lecture 7: Backpropagation (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point) (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum) (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate) (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響 (youtube.com) 【機器學習2021】類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介 (youtube.com) Notes ML or DL 都可以理解为三个步骤： Define a function set: Deep Learning(CNN, Transformer), Decision Tree, etc. Goodness of function: Supervised Learning, Semi-supervised Learning, Reinforced Learning, etc. Pick the best function: Gradient Descent(Adam, AdamW), Genetic Algorithm, etc. Gradient Descent 就是用总的 Loss 对网络中每个权重和偏置求偏导，然后用这个参数减去即可，即 $w_{1}-\\mu \\frac{\\partial L}{\\partial w_{1}}$ ，其中 $\\mu$ 表示 Learning Rate，通常来说，$\\eta$ 要么为一开始的固定值，要么为一个关于 $t$ 的函数，并且通常随 $t$ 增大而变小 训练模型时，最后把 epochs / loss 的折线图输出，方便观察当前 learning rate 下参数变化导致的 loss 变化趋势，以便调整 learning rate Learning Rate 不仅跟一阶微分成正比，还和二阶微分成反比，即 $\\frac{ \\left| First \\space Derivative \\right|}{Second \\space Derivative}$ Adaptive Learning Rates 就是说针对每个参数都给一个独立的 Learning Rate，比如 Adagrad，将原始的参数更新式修改为 $w_{t}-\\frac{\\eta^{t}}{\\sigma^{t}}g^{t}$ ，其中 $\\sigma^t$ : root mean square of the previous derivatives of parameter $w$ ，$\\eta^{t}$ : time $t$’s learning rate，$\\sigma^t=\\sqrt{\\frac{1}{t+1}\\sum_{i=0}^{t}(g^{i})^{2}}$ ，Adagrad 反映了每次的梯度的反差大小 Stochastic Gradient Descent 简单来说就是不求所有 sample 的 loss 和，随机选择一个 sample 计算它的 loss 和 gradient 作为梯度下降的依据，更新所有参数；传统梯度下降会把 $n$ samples 看一遍然后更新一次，这种做法看完 $n$ samples 后总共会更新 $n$ 次 对于数据做 Feature Scaling(Normalization) 是有必要的，这可以使训练时的 learning rate 相对统一（每一种数据对参数的影响相当），通常 Normalization 的做法时 $x_i^r\\leftarrow\\frac{x_i^r-m_i}{\\sigma_i}$ ，其中 $m_{i}$ 时均值，$\\sigma_{i}$ 是标准差 梯度下降的数理基础是一阶泰勒展开，假设当前有两个参数 $a$ 和 $b$ ，这俩参数将要修改为 $\\theta_1$ 和 $\\theta_2$ ，则新的 loss 值可以表示为 $$\\mathrm{L}\\big(\\theta\\big)\\approx\\mathrm{L}\\big(a,b\\big)+\\frac{\\partial\\mathrm{L}\\big(a,b\\big)}{\\partial\\theta_1}\\big(\\theta_1-a\\big)+\\frac{\\partial\\mathrm{L}\\big(a,b\\big)}{\\partial\\theta_2}\\big(\\theta_2-b\\big)$$ 只需要找这个被泰勒展开后的式子的最小值即可，但这个 $\\approx$ 成立的前提是你的移动值域范围够小，也就是 learning rate 要足够小 Backpropagation 核心原理是链式法则 Chain Rule，因为在使用 Gradient Descent 的时候，下式中对 $w$ 的偏导是不容易被直接计算的： $$L(\\theta)=\\sum_{n=1}^NC^n(\\theta)\\quad\\longrightarrow\\quad\\frac{\\partial L(\\theta)}{\\partial w}=\\sum_{n=1}^N\\frac{\\partial C^n(\\theta)}{\\partial w}$$ 通过使用链式法则，可以变形为如下： $$\\frac{\\partial C}{\\partial w}=\\frac{\\partial z}{\\partial w}\\frac{\\partial C}{\\partial z},\\quad\\frac{\\partial C}{\\partial z}=\\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a},\\quad\\frac{\\partial C}{\\partial a}=\\frac{\\partial z^{\\prime}}{\\partial a}\\frac{\\partial C}{\\partial z^{\\prime}}+\\frac{\\partial z^{\\prime\\prime}}{\\partial a}\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$$可以发现，这个求导的过程是递归的，因此可以建一个反图，从最后的 Output Layer 开始使用梯度优化参数 局部最小/最大，或者鞍点等，梯度都会为 0，此时 loss 值不会减小，通过对 loss 函数进行二次泰勒展开，可以发现通过二次微分项能判断是否是鞍点，也就是 Hession 矩阵，如果 H 矩阵的特征值全为正数，那么是局部最小；全为负数，则是局部最大；有正有负，则是鞍点 Batch Size 不是越大越好，也不是越小越好，如下图： Momentum 做法是将物理中小球滚下斜坡的思想引入，每次 Gradient Descent 的时候考虑上一步的方向，又或者说，当前移动的方向是现在到过去所有 Gradient 的加权和 Adam: RMSProp + Momentum Recurrent Neural NetworkCourses ML Lecture 21-1: Recurrent Neural Network (Part I) (youtube.com) ML Lecture 21-2: Recurrent Neural Network (Part II) (youtube.com) Notes RNN 简单来说就是原本 Neural Network 中的第 $i$ 层只由 $i-1$ 层的 output 作为 input，现在还接入了上一时刻 $t-1$ 加入这次的 input，形式化的说 $$\\mathbf{C}_{i}^{t}= \\mathcal{F}_{i}(\\mathbf{C}_{i}^{t-1}, \\space \\mathbf{Output}_{i-1}^{t})$$ LSTM（Long Short-term Memory）是现在最普遍的 RNN 形式，具体来说，对于一个 Neural，会有三个 Gate 去控制它的 Input、Output、Forget 的程度，如下图","link":"/2024/07/22/Machine-Learning-Notes/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/04/30/hello-world/"},{"title":"ML2023Spring-hw1","text":"Regression TaskLink: ML2023Spring-hw1 | Kaggle 本次 homework 的任务是根据 COVID-19 Cases 进行 Prediction，主要使用了线性全连接层和 ReLU 激活函数构建网络 为了达到更高的 Baseline 采用了特征截取、Adam 学习器、L2 正则化（weight decay in optimizer）并在 Sample 代码基础上增加了网络深度 Network StructureBrief Intro1 input layer (implicit) 3 linear layers (with ReLU activation function) 1 output layer Core Code1234567891011121314151617class LinearModel(nn.Module): def __init__(self, inputs): super(LinearModel, self).__init__() self.layer1 = nn.Sequential( nn.Linear(inputs, 16), nn.ReLU(), nn.Linear(16, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 1) ) def forward(self, x): x = self.layer1(x) x = x.squeeze(1) # (X, 1) -&gt; (X) return x Learned Things这篇笔记是事后补的，记忆有些凌乱，以下记录不具有任何逻辑顺序 事先定义一个设置随机数种子的函数以确保每次运行结果一致 有些用于评估的变量记得添加 detach 使其从计算图中分离 数据读入三步走：划分训练集和验证集、将 data 和 label 通过 Dataset 打包、通过 DataLoader 将 dataset 创建维 loader 供训练使用 epochs 较多时可以采用早停法提高训练效率 数据提前进行特征数据筛选/截取，将明显无关的数据扔掉 Full CodePrivate Score: 0.84523 Public score: 0.82729 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210# Numerical Operationsimport mathimport numpy as np# Reading/Writing Dataimport pandas as pdimport osimport csv# For Progress Barfrom tqdm import tqdm# Pytorchimport torch import torch.nn as nnfrom torch.utils.data import Dataset, DataLoader, random_split# For plotting learning curvefrom torch.utils.tensorboard import SummaryWriterclass MyDataset(Dataset): def __init__(self, x, y=None): if y is not None: self.y = torch.tensor(y, dtype=torch.float32) self.x = torch.tensor(x, dtype=torch.float32) def __getitem__(self, index): if hasattr(self, 'y'): return self.x[index], self.y[index] else: return self.x[index] def __len__(self): return len(self.x) class LinearModel(nn.Module): def __init__(self, inputs): super(LinearModel, self).__init__() self.layer1 = nn.Sequential( nn.Linear(inputs, 16), nn.ReLU(), nn.Linear(16, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 1) ) def forward(self, x): x = self.layer1(x) x = x.squeeze(1) # (X, 1) -&gt; (X) return xdef set_seed(seed): np.random.seed(seed) torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)def split_data(dataset, valid_rate, seed): valid_size = int(valid_rate * len(dataset)) train_size = len(dataset) - valid_size train_set, valid_set = random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(seed)) return np.array(train_set), np.array(valid_set)def predict(test_loader, model, device): model.eval() preds = [] for x in tqdm(test_loader): x = x.to(device) with torch.no_grad(): pred = model(x) preds.append(pred.detach().cpu()) # detach 的作用是将张量从这个计算图中分离出来， # 使其不再跟踪计算图中的操作 preds = torch.cat(preds, dim=0).numpy() return predsdef select_feat(train_data, valid_data, test_data, select_all=True): y_train, y_valid = train_data[:,-1], valid_data[:,-1] raw_x_train, raw_x_valid, raw_x_test = train_data[:, :-1], valid_data[:, :-1], test_data if select_all: feat_idx = list(range(raw_x_train.shape[1])) else: feat_idx = list(range(35, raw_x_train.shape[1])) return raw_x_train[:, feat_idx], raw_x_valid[:, feat_idx], raw_x_test[:, feat_idx], y_train, y_validdef train_model(train_loader, valid_loader, model, args, device): critertion = nn.MSELoss(reduction='mean') optimizer = torch.optim.Adam(model.parameters(), lr=args['learning_rate'], weight_decay=1e-4) writer = SummaryWriter() if not os.path.exists('./models'): os.mkdir('./models') epochs, best_loss, step, early_stop = args['epochs'], math.inf, 0, 0 for i in range(epochs): model.train() loss_sum = [] tarin_process_bar = tqdm(train_loader, position=0, leave=True) for x, y in tarin_process_bar: optimizer.zero_grad() x, y = x.to(device), y.to(device) pred = model(x) loss = critertion(pred, y) loss.backward() optimizer.step() step += 1 loss_sum.append(loss.detach().item()) tarin_process_bar.set_description(f'Epoch [{i+1}/{epochs}]') tarin_process_bar.set_postfix({'loss': loss.detach().item()}) mean_train_loss = sum(loss_sum) / len(loss_sum) writer.add_scalar('loss/train', mean_train_loss, step) model.eval() loss_sum = [] for x, y in valid_loader: x, y = x.to(device), y.to(device) with torch.no_grad(): pred = model(x) loss = critertion(pred, y) loss_sum.append(loss.item()) mean_valid_loss = sum(loss_sum) / len(loss_sum) print(f'Epoch [{i+1}/{epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}') writer.add_scalar('loss/valid', mean_valid_loss, step) if mean_valid_loss &lt; best_loss: best_loss = mean_valid_loss torch.save(model.state_dict(), args['save_path']) print(f'Saving model with loss {best_loss:.3f}') early_stop = 0 else: early_stop += 1 if (early_stop == args['early_stop']): print('\\nModel is not improving, so we stop training') return def save_pred(preds, file): ''' Save predictions to specified file ''' with open(file, 'w', newline='') as fp: writer = csv.writer(fp) writer.writerow(['id', 'tested_positive']) for i, p in enumerate(preds): writer.writerow([i, p])def main(args): device = 'cuda' if torch.cuda.is_available() else 'cpu' set_seed(args['seed']) # train_data size: 3009 x 89 # test_data size: 997 x 88 train_data, test_data = pd.read_csv('./covid_train.csv').values, pd.read_csv('./covid_test.csv').values train_data, valid_data = split_data(train_data, args['valid_ratio'], args['seed']) print(f'train_data size: {train_data.shape}\\nvalid_data size: {valid_data.shape}\\ntest_data size: {test_data.shape}') x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, args['select_all']) train_dataset, valid_dataset, test_dataset = MyDataset(x_train, y_train), \\ MyDataset(x_valid, y_valid), \\ MyDataset(x_test) # Pytorch data loader loads pytorch dataset into batches. train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, pin_memory=True) valid_loader = DataLoader(valid_dataset, batch_size=args['batch_size'], shuffle=True, pin_memory=True) test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False, pin_memory=True) # print(train_loader) model = LinearModel(inputs=x_train.shape[1]).to(device) train_model(train_loader, valid_loader, model, args, device) model.load_state_dict(torch.load(args['save_path'])) output = predict(test_loader, model, device) print(output) save_pred(output, 'pred.csv') if __name__ == '__main__': args = { 'seed': 42, 'select_all': False, 'valid_ratio': 0.2, 'epochs': 3000, 'batch_size': 128, 'learning_rate': 1e-3, 'early_stop': 600, 'save_path': './models/model.ckpt' } main(args)","link":"/2024/07/24/ML2023Spring-hw1/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Study","slug":"Study","link":"/tags/Study/"},{"name":"Contest","slug":"Contest","link":"/tags/Contest/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"}],"categories":[{"name":"Machine Learning Course","slug":"Machine-Learning-Course","link":"/categories/Machine-Learning-Course/"},{"name":"XCPC","slug":"XCPC","link":"/categories/XCPC/"},{"name":"Codeforces","slug":"XCPC/Codeforces","link":"/categories/XCPC/Codeforces/"}],"pages":[]}